{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import tempfile, cv2, numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 7, padding=3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        return x.mean(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f57ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsensusTransformer(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4)\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoldenGrounding(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.relu(out).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48acbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1024):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.opinion = nn.Linear(128, 3)\n",
    "        self.emotion = nn.Linear(128, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.opinion(x), self.emotion(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1baa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCCSAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.video_encoder = VideoEncoder()\n",
    "        self.consensus = ConsensusTransformer()\n",
    "        self.golden = GoldenGrounding()\n",
    "        self.fusion = FusionClassifier()\n",
    "\n",
    "    def forward(self, video_feats, text_feats):\n",
    "        v = self.video_encoder(video_feats.permute(0, 2, 1))\n",
    "        t = text_feats\n",
    "        combined = torch.cat((v, t), dim=1)\n",
    "        return self.fusion(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_video(video_file, max_frames=16):\n",
    "    tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')\n",
    "    tfile.write(video_file.read())\n",
    "    cap = cv2.VideoCapture(tfile.name)\n",
    "    frames, count = [], 0\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    rate = max(total // max_frames, 1)\n",
    "    while cap.isOpened() and len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        if count % rate == 0:\n",
    "            f = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            f = cv2.resize(f, (64, 64))\n",
    "            frames.append(f)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    return torch.tensor(np.stack(frames)/255.0, dtype=torch.float32).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(text, tokenizer, model):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**tokens)\n",
    "    return out.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(video_file, comment):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "    model = VCCSAModel().to(device)\n",
    "    model.eval()\n",
    "    v = extract_frames_from_video(video_file)\n",
    "    t = extract_text_features(comment, tokenizer, text_model)\n",
    "    o_logits, e_logits = model(v, t)\n",
    "    opinion = o_logits.argmax(1).item()\n",
    "    emotion = e_logits.argmax(1).item()\n",
    "    return opinion, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "emotion_labels = [\"Fear\", \"Disgust\", \"Anger\", \"Sadness\", \"Joy\", \"Trust\", \"Anticipation\", \"Surprise\"]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}